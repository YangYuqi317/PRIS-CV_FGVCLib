# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, yyq
# This file is distributed under the same license as the FGVClib package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: FGVClib \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-11-11 13:06+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../en/configs/pmg_resnet50/README.md:1
#: ../../en/configs/pmg_resnet50/README.md:3
#: 11a13bd8c7e742eab64c886cd81b5912
#: cddeb57c16a243a3915140d904ec04d5
msgid "PMG"
msgstr ""

#: ../../en/configs/pmg_resnet50/README.md:5
#: da227972dea04b3586790635fef94a0f
msgid "Abstract"
msgstr ""

#: ../../en/configs/pmg_resnet50/README.md:6
#: 6ddede0c86d2432497f2ec7103bfa9a6
msgid "In this work, we propose a novel framework for fine-grained visual classification to tackle these problems. In particular, we propose: (i) a progressive training strategy that effectively fuses features from different granularities, and (ii) a random jigsaw patch generator that encourages the network to learn features at specific granularities. We obtain state-of-the-art performances on several standard FGVC benchmark datasets, where the proposed method consistently outperforms existing methods or delivers competitive results."
msgstr ""

#: ../../en/configs/pmg_resnet50/README.md:12
#: c41a0447d4764e96af7b3d0e89a8d4da
msgid "Illustration of features learned by general methods (a and b) and our proposed method (c and d). (a) Traditional convolution neural networks trained with cross entropy (CE) loss tend to find the most discriminative parts. (b) Other state-of-the-art methods focus on how to find more discriminative parts. (c) Our proposed progressive training (Here we use last three stages for explanation.) gradually locates discriminative information from low stages to deep stage. And features extracted from all trained stages are concatenated together to ensure complementary relationships are fully explored, which is represented by “Stage Concat.” (d) With assistance of jigsaw puzzle generator the granularity of parts learned at each step are restricted inside patches."
msgstr ""

#: ../../en/configs/pmg_resnet50/README.md:18
#: d9f83ae77fdb4d3e99863e769e26a44d
msgid "Introduction"
msgstr ""

#: ../../en/configs/pmg_resnet50/README.md:19
#: e6ad6db8abc34f2cad0b912cba0dfa51
msgid "Progressive training methodology was originally proposed for generative adversarial networks, where it started with low-resolution images, and then progressively increased the resolution by adding layers to the networks. Instead of learning the information from all the scales, this strategy allows the network to discover large-scale structure of the image distribution and then shift attention to increasingly ner scale details. Recently, progressive training strategy has been widely utilized for generation tasks, since it can simplify the information propagation within the network by intermediate supervision. For FGVC, the fusion of multi-granularity information is critical to the model performance. In this work, we adopt the idea of progressive training to design a single network that can learn these information with a series of training stages. The input images are firstly split into small patches to train a low-level layers of model. Then the number of patches are progressively increased and the corresponding layers high-level lays have been added and trained, correspondingly. Most of the existing work with progressive training are focusing on the task of sample generation. To the best of our knowledge, it has not been attempted earlier for the task of FGVC."
msgstr ""

#: ../../en/configs/pmg_resnet50/README.md:25
#: 5e7c31b751f64ffaa0b050a0ba37797c
msgid "The training procedure of the progressive training which consists of S + 1 steps at each iteration (Here S = 3 for explanation). The Conv Block represents the combination of two convolution layers with and max pooling layer, and Classif ier represent two fully connected layers with a softmax layer at the end. At each iteration, the training data are augmented by the jigsaw generator and sequentially input into the network by S + 1 steps. In our training process, the hyper-parameter n is 2L−l+1 for the lth stage. At each step, the output from the corresponding classifier will be used for loss computation and parameter updating."
msgstr ""
